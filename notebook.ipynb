{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eca8f7",
   "metadata": {},
   "source": [
    "## **Pendahuluan**\n",
    "\n",
    "Pada praktikum ini, kita akan membangun sebuah model klasifikasi gambar menggunakan TensorFlow dan Keras.\n",
    "Model ini akan digunakan untuk mengklasifikasikan gambar menjadi tiga kategori: **Rock, Paper, dan Scissors**.\n",
    "\n",
    "Agar proses pelatihan menjadi lebih efisien dan cepat, kita akan memanfaatkan teknik **Transfer Learning** dengan menggunakan model pre-trained **MobileNetV2** sebagai feature extractor.\n",
    "\n",
    "### **Dataset**\n",
    "\n",
    "Dataset yang digunakan dalam praktikum ini berasal dari Kaggle, dan dapat diunduh melalui tautan berikut:\n",
    "\n",
    "🔗 [Rock-Paper-Scissors Dataset – Kaggle](https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors)\n",
    "\n",
    "Setelah mendownload dataset dan mengekstraknya, **praktikan diwajibkan** untuk menyusun struktur folder dataset menjadi seperti berikut:\n",
    "\n",
    "<pre>\n",
    "dataset/\n",
    "    rock/\n",
    "    paper/\n",
    "    scissor/\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a03446",
   "metadata": {},
   "source": [
    "### **Load Dataset**\n",
    "---\n",
    "\n",
    "Kode di bawah ini digunakan untuk memuat dataset citra dari struktur folder yang telah disiapkan sebelumnya.\n",
    "\n",
    "Praktikan diminta untuk:\n",
    "\n",
    "- Menentukan **ukuran gambar** (`img_size`) sesuai dengan model yang akan digunakan (misalnya 224x224 untuk MobileNetV2).\n",
    "- Mengatur **ukuran batch** (`batch_size`) sesuai kapasitas RAM/VRAM masing-masing.\n",
    "- Mengisi nilai **`seed`** untuk memastikan bahwa hasil pembagian data bisa direproduksi kembali dengan urutan yang sama setiap kali dijalankan.\n",
    "\n",
    "Dataset akan dibagi menjadi tiga bagian:\n",
    "- **Training set**: 70%\n",
    "- **Validation set**: 15%\n",
    "- **Testing set**: 15%\n",
    "\n",
    "Proses pemuatan data dilakukan dengan menggunakan fungsi bawaan TensorFlow, yaitu `image_dataset_from_directory()`, yang akan secara otomatis membaca gambar berdasarkan nama subfoldernya sebagai label.\n",
    "\n",
    "> **Catatan:**\n",
    "> Praktikan diperbolehkan untuk **menyesuaikan sendiri persentase pembagian data** (misalnya 80% train, 10% val, 10% test) selama proporsinya tetap konsisten dan masuk akal.  \n",
    "> Praktikan juga dapat mengubah **ukuran gambar (`img_size`)** jika ingin menggunakan arsitektur model yang berbeda, serta **mengganti nilai `seed`** untuk mencoba hasil pembagian data yang berbeda.\n",
    "\n",
    "Setelah training set dan validation+test set dimuat, validation dan test akan dipisahkan secara manual berdasarkan fraksi yang telah ditentukan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7363874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2188 files belonging to 3 classes.\n",
      "Using 1532 files for training.\n",
      "Found 2188 files belonging to 3 classes.\n",
      "Using 656 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "\n",
    "# === PARAMETER YANG HARUS DITENTUKAN PRAKTIKAN ===\n",
    "dataset_dir = \"dataset\"          # folder utama dataset\n",
    "img_size    = (224, 224)         # ukuran gambar (standard for MobileNetV2)\n",
    "batch_size  = 32                 # ukuran batch\n",
    "seed        = 42                 # random seed untuk reproducibility\n",
    "\n",
    "# Dapatkan daftar kelas dari folder\n",
    "classes = sorted(os.listdir(dataset_dir))\n",
    "\n",
    "# === ATURAN SPLIT DATA ===\n",
    "train_frac = 0.70\n",
    "val_frac   = 0.15\n",
    "test_frac  = 0.15\n",
    "vt_frac    = val_frac + test_frac   # val + test (untuk split manual nanti)\n",
    "\n",
    "# === MEMUAT TRAIN DATASET ===\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    validation_split=vt_frac,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",        # gunakan one-hot encoding\n",
    ")\n",
    "\n",
    "# === MEMUAT VALIDATION + TEST DATASET (gabungan) ===\n",
    "val_test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    validation_split=vt_frac,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "# === SPLIT VAL DAN TEST SECARA MANUAL ===\n",
    "vt_batches  = tf.data.experimental.cardinality(val_test_ds).numpy()\n",
    "val_batches = int(vt_batches * (val_frac / vt_frac))   # hitung proporsi val dalam val+test\n",
    "\n",
    "val_ds  = val_test_ds.take(val_batches)\n",
    "test_ds = val_test_ds.skip(val_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f393e",
   "metadata": {},
   "source": [
    "### **Preprocessing dan Augmentasi Data**\n",
    "---\n",
    "\n",
    "Sebelum gambar dapat digunakan untuk melatih model, dataset perlu melalui proses **preprocessing** terlebih dahulu. Preprocessing ini bertujuan untuk memastikan bahwa semua input gambar berada dalam skala nilai yang konsisten dan sesuai dengan ekspektasi dari model.\n",
    "\n",
    "Pada praktikum ini, preprocessing dilakukan melalui dua tahap:\n",
    "\n",
    "1. **Normalisasi**:  \n",
    "   Setiap pixel gambar yang awalnya berada pada rentang nilai **[0, 255]** diubah ke rentang **[0, 1]** dengan menggunakan layer `Rescaling(1./255)`.\n",
    "\n",
    "2. **Augmentasi** (opsional):  \n",
    "   Untuk meningkatkan variasi data dan mengurangi overfitting, dilakukan augmentasi pada **data training saja**. Augmentasi dilakukan melalui transformasi berikut:\n",
    "   - `RandomFlip(\"horizontal\")`: membalik gambar secara horizontal.\n",
    "   - `RandomRotation(0.1)`: memutar gambar secara acak hingga 10%.\n",
    "   - `RandomZoom(0.1, 0.1)`: melakukan zoom in/out hingga 10%.\n",
    "   - `RandomContrast(0.1)`: mengubah kontras gambar secara acak hingga ±10%.\n",
    "\n",
    "Semua preprocessing dan augmentasi dilakukan melalui fungsi `prep()`.  \n",
    "Fungsi ini menerima parameter `augment` untuk menentukan apakah augmentasi akan diterapkan atau tidak.  \n",
    "- Jika `augment=True`, maka data akan dinormalisasi lalu diaugmentasi.  \n",
    "- Jika `augment=False`, maka hanya normalisasi yang dilakukan (untuk validation dan test set).\n",
    "\n",
    "Setelah diproses, setiap dataset akan di-cache dan di-prefetch menggunakan `AUTOTUNE` untuk mengoptimalkan performa saat training.\n",
    "\n",
    "> **Catatan untuk praktikan:**  \n",
    "> Praktikan dapat menyesuaikan jenis dan tingkat augmentasi pada `aug_layer` untuk melihat pengaruhnya terhadap performa model. Namun, augmentasi **hanya boleh diterapkan pada training set**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a16769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREPROCESSING DAN AUGMENTASI DATA ===\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# praktikan diminta melengkapi isi preprocessing dan augmentasi\n",
    "norm_layer = tf.keras.Sequential([\n",
    "    layers.Rescaling(1./255),  # normalisasi ke [0,1]\n",
    "])\n",
    "\n",
    "aug_layer = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),    # flip kiri-kanan\n",
    "    layers.RandomRotation(0.1),         # rotasi ±10%\n",
    "    layers.RandomZoom(0.1, 0.1),        # zoom in/out sampai 10%\n",
    "    layers.RandomContrast(0.1),         # kontras +-10%\n",
    "])\n",
    "\n",
    "# fungsi untuk mempersiapkan dataset\n",
    "def prep(ds, augment=False):\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x, y: (aug_layer(norm_layer(x), training=True), y),\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        ds = ds.map(lambda x, y: (norm_layer(x), y),\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Praktikan diminta menentukan kapan augmentasi diaktifkan\n",
    "# Augmentasi hanya diaktifkan pada training set, tidak pada validation dan test\n",
    "train_ds = prep(train_ds, augment=True)   # Augmentasi diaktifkan untuk training data\n",
    "val_ds   = prep(val_ds,   augment=False)  # Hanya normalisasi untuk validation\n",
    "test_ds  = prep(test_ds,  augment=False)  # Hanya normalisasi untuk test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a1510",
   "metadata": {},
   "source": [
    "### **Build Model dan Training**\n",
    "---\n",
    "\n",
    "Pada tahap ini, kita akan membangun arsitektur model menggunakan teknik **Transfer Learning** dengan memanfaatkan **MobileNetV2** sebagai feature extractor.\n",
    "\n",
    "Praktikan diminta untuk:\n",
    "\n",
    "- **Melengkapi input layer** (`inputs`) dengan ukuran shape gambar yang sesuai.\n",
    "- **Melengkapi output layer** (`outputs`) dengan jumlah neuron yang sama dengan jumlah kelas target (jumlah kategori pada dataset).\n",
    "\n",
    "Bagian head klasifikasi model disusun dengan struktur:\n",
    "\n",
    "- **GlobalAveragePooling2D**: Mengubah output feature map menjadi satu vektor rata-rata per channel.\n",
    "- **Dropout**: Teknik regularisasi untuk mengurangi overfitting.\n",
    "- **Dense (Softmax)**: Layer output yang mengklasifikasikan input ke dalam salah satu kelas.\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Model**\n",
    "\n",
    "Sebelum memulai proses pelatihan, kita juga telah mendefinisikan dua buah callbacks:\n",
    "\n",
    "- `EarlyStopping`: Untuk menghentikan pelatihan jika validation loss tidak membaik setelah sejumlah epoch tertentu.\n",
    "- `ModelCheckpoint`: Untuk menyimpan model terbaik berdasarkan validation loss.\n",
    "\n",
    "Praktikan diminta untuk:\n",
    "\n",
    "- **Melengkapi list callbacks** di fungsi `model.fit()`, dengan menambahkan kedua callbacks yang telah dibuat sebelumnya (`early_stopping` dan `model_checkpoint`).\n",
    "- Menjalankan training selama **50 epoch** atau hingga proses berhenti otomatis karena `EarlyStopping`.\n",
    "\n",
    "> **Catatan:**  \n",
    "> Dengan menggunakan callbacks, kita dapat menghindari overfitting dan memastikan model terbaik tersimpan otomatis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575df2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977ms/step - accuracy: 0.5821 - loss: 0.8690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 1s/step - accuracy: 0.5858 - loss: 0.8628 - val_accuracy: 0.9625 - val_loss: 0.1647\n",
      "Epoch 2/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793ms/step - accuracy: 0.9542 - loss: 0.1878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 973ms/step - accuracy: 0.9543 - loss: 0.1874 - val_accuracy: 0.9812 - val_loss: 0.0871\n",
      "Epoch 3/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801ms/step - accuracy: 0.9719 - loss: 0.1117"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 987ms/step - accuracy: 0.9718 - loss: 0.1117 - val_accuracy: 0.9844 - val_loss: 0.0606\n",
      "Epoch 4/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799ms/step - accuracy: 0.9881 - loss: 0.0725"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 971ms/step - accuracy: 0.9880 - loss: 0.0726 - val_accuracy: 0.9875 - val_loss: 0.0497\n",
      "Epoch 5/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779ms/step - accuracy: 0.9858 - loss: 0.0616"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 954ms/step - accuracy: 0.9858 - loss: 0.0616 - val_accuracy: 0.9875 - val_loss: 0.0434\n",
      "Epoch 6/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783ms/step - accuracy: 0.9913 - loss: 0.0508"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 956ms/step - accuracy: 0.9912 - loss: 0.0508 - val_accuracy: 0.9875 - val_loss: 0.0366\n",
      "Epoch 7/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - accuracy: 0.9889 - loss: 0.0426"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 979ms/step - accuracy: 0.9890 - loss: 0.0426 - val_accuracy: 0.9875 - val_loss: 0.0348\n",
      "Epoch 8/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 786ms/step - accuracy: 0.9948 - loss: 0.0372"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 956ms/step - accuracy: 0.9947 - loss: 0.0372 - val_accuracy: 0.9875 - val_loss: 0.0315\n",
      "Epoch 9/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796ms/step - accuracy: 0.9969 - loss: 0.0303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 963ms/step - accuracy: 0.9969 - loss: 0.0304 - val_accuracy: 0.9906 - val_loss: 0.0261\n",
      "Epoch 10/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 937ms/step - accuracy: 0.9970 - loss: 0.0241 - val_accuracy: 0.9875 - val_loss: 0.0264\n",
      "Epoch 11/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774ms/step - accuracy: 0.9994 - loss: 0.0208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 944ms/step - accuracy: 0.9994 - loss: 0.0209 - val_accuracy: 0.9875 - val_loss: 0.0250\n",
      "Epoch 12/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761ms/step - accuracy: 0.9974 - loss: 0.0199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 934ms/step - accuracy: 0.9974 - loss: 0.0199 - val_accuracy: 0.9906 - val_loss: 0.0225\n",
      "Epoch 13/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 948ms/step - accuracy: 0.9980 - loss: 0.0189 - val_accuracy: 0.9906 - val_loss: 0.0238\n",
      "Epoch 14/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769ms/step - accuracy: 0.9961 - loss: 0.0183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 937ms/step - accuracy: 0.9961 - loss: 0.0183 - val_accuracy: 0.9906 - val_loss: 0.0211\n",
      "Epoch 15/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761ms/step - accuracy: 0.9989 - loss: 0.0140"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 929ms/step - accuracy: 0.9989 - loss: 0.0140 - val_accuracy: 0.9906 - val_loss: 0.0193\n",
      "Epoch 16/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 936ms/step - accuracy: 0.9982 - loss: 0.0158 - val_accuracy: 0.9906 - val_loss: 0.0200\n",
      "Epoch 17/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776ms/step - accuracy: 0.9984 - loss: 0.0148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 942ms/step - accuracy: 0.9984 - loss: 0.0148 - val_accuracy: 0.9937 - val_loss: 0.0174\n",
      "Epoch 18/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 957ms/step - accuracy: 0.9987 - loss: 0.0116 - val_accuracy: 0.9906 - val_loss: 0.0209\n",
      "Epoch 19/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 932ms/step - accuracy: 0.9961 - loss: 0.0149 - val_accuracy: 0.9906 - val_loss: 0.0191\n",
      "Epoch 20/50\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 935ms/step - accuracy: 0.9996 - loss: 0.0111 - val_accuracy: 0.9906 - val_loss: 0.0189\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Load pre-trained base\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(*img_size, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "# Tambahkan head klasifikasi baru\n",
    "# Praktikan diminta melengkapi bagian input dan output di bawah ini\n",
    "# Lengkapi bagian yang kosong dengan menyusun arsitektur head klasifikasi\n",
    "inputs = tf.keras.Input(shape=(*img_size, 3))    # Tentukan input shape\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(len(classes), activation=\"softmax\")(x)  # Tentukan jumlah output classes\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer  = \"adam\",\n",
    "    loss       = \"categorical_crossentropy\",\n",
    "    metrics    = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor    = \"val_loss\",\n",
    "    patience   = 3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model_checkpoint = callbacks.ModelCheckpoint(\n",
    "    \"model/best_transfer.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Training model\n",
    "# Praktikan diminta melengkapi callbacks yang digunakan\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, model_checkpoint]    # Lengkapi dengan callbacks yang telah dibuat\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957bf440",
   "metadata": {},
   "source": [
    "### **Evaluasi Model**\n",
    "---\n",
    "\n",
    "Setelah model selesai dilatih, langkah berikutnya adalah mengevaluasi performanya pada **test set**.\n",
    "\n",
    "Evaluasi dilakukan menggunakan dua pendekatan utama:\n",
    "\n",
    "1. **Classification Report**  \n",
    "   Menampilkan metrik klasifikasi seperti:\n",
    "   - **Precision**: Seberapa akurat model saat membuat prediksi positif.\n",
    "   - **Recall**: Seberapa baik model menangkap semua sampel positif.\n",
    "   - **F1-Score**: Harmoni antara precision dan recall.\n",
    "   - **Accuracy**: Persentase prediksi yang benar dari semua prediksi.\n",
    "\n",
    "2. **Confusion Matrix**  \n",
    "   Menunjukkan jumlah prediksi benar dan salah untuk masing-masing kelas, dalam bentuk tabel.  \n",
    "   Dengan confusion matrix, kita bisa melihat pola kesalahan model (misal, apakah model sering salah membedakan \"rock\" dengan \"paper\", dll).\n",
    "\n",
    "---\n",
    "\n",
    "#### Langkah-langkah yang dilakukan pada kode:\n",
    "\n",
    "- Menggunakan model untuk melakukan prediksi pada seluruh test dataset.\n",
    "- Membandingkan hasil prediksi dengan label asli.\n",
    "- Menampilkan classification report menggunakan `classification_report` dari scikit-learn.\n",
    "- Menghitung dan memvisualisasikan confusion matrix menggunakan `ConfusionMatrixDisplay`.\n",
    "\n",
    "> **Catatan untuk praktikan:**  \n",
    "> Perhatikan nilai **precision**, **recall**, dan **f1-score** dari masing-masing kelas.  \n",
    "> Jika terdapat ketidakseimbangan performa antar kelas, analisis kemungkinan penyebabnya (misalnya karena jumlah data tidak seimbang, kesamaan visual antar kelas, dsb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b89597f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 891ms/step\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       paper     0.9882    0.9655    0.9767        87\n",
      "        rock     0.9919    0.9919    0.9919       124\n",
      "    scissors     0.9764    0.9920    0.9841       125\n",
      "\n",
      "    accuracy                         0.9851       336\n",
      "   macro avg     0.9855    0.9832    0.9843       336\n",
      "weighted avg     0.9852    0.9851    0.9851       336\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed and saved as 'best_transfer.h5'\n",
      "This model can be used with FastAPI or Streamlit for developing a classification app.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# dapatkan prediksi probabilitas dan label dari test set\n",
    "y_pred_probs = model.predict(test_ds)               # shape = (N, num_classes)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)             # ambil indeks prediksi tertinggi (shape: (N,))\n",
    "\n",
    "# ekstrak label asli dari test_ds\n",
    "# Karena test_ds bertipe (x_batch, y_batch) dengan label one-hot, kita ambil argmax dari y_batch\n",
    "y_true = np.concatenate([\n",
    "    np.argmax(y.numpy(), axis=1)\n",
    "    for _, y in test_ds\n",
    "], axis=0)\n",
    "\n",
    "# tampilkan classification report\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=classes,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# hitung dan tampilkan confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "disp.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Model training completed and saved as 'best_transfer.h5'\")\n",
    "print(\"This model can be used with FastAPI or Streamlit for developing a classification app.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e376bd",
   "metadata": {},
   "source": [
    "Setelah proses pelatihan selesai, model yang telah dilatih akan secara otomatis disimpan oleh callback `ModelCheckpoint` ke dalam file `.h5`.\n",
    "\n",
    "Model ini dapat langsung digunakan dalam pengembangan aplikasi backend yang menggunakan **FastAPI** atau frontend berbasis **Streamlit**, untuk membuat aplikasi klasifikasi gambar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
